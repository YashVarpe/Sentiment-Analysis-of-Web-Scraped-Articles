{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk \n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'OutputDataStructure.csv'\n",
    "df = pd.read_csv(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        words = text.split()\n",
    "        return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_directory(directory_path):\n",
    "    file_word_counts = {}       # Dictionary is created\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            word_count = count_words_in_file(file_path)\n",
    "            file_word_counts[filename] = word_count     # Key : Value pair\n",
    "    return file_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output'\n",
    "file_word_counts = count_words_in_directory(directory_path)\n",
    "\n",
    "for filename, word_count in file_word_counts.items():       # To print dictionary items i.e key : value pair\n",
    "    print(f'{filename}: {word_count} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_word_counts:\n",
    "        df.loc[index, 'WORD COUNT'] = file_word_counts[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'output'\n",
    "file_word_counts = count_words_in_directory(directory_path)\n",
    "\n",
    "for filename, total_words in file_word_counts.items():\n",
    "    print(f'{filename}: {total_words} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_word_length(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        words = text.split()\n",
    "        total_word_length = sum(len(word) for word in words)\n",
    "        total_words = len(words)\n",
    "        average_word_length = total_word_length / total_words if total_words > 0 else 0\n",
    "        return average_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path):\n",
    "    file_averages = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            average_word_length = calculate_average_word_length(file_path)\n",
    "            file_averages[filename] = average_word_length\n",
    "    return file_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output' \n",
    "file_averages = process_files_in_directory(directory_path)\n",
    "\n",
    "for filename, average_word_length in file_averages.items():\n",
    "    print(f\"{filename}: average word length: {average_word_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\"  \n",
    "    if filename in file_averages:\n",
    "        df.loc[index, 'AVG WORD LENGTH'] = file_averages[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total sentences for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentences_in_directory(directory_path):\n",
    "    sentences_per_file = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            num_sentences = count_sentences(file_path)\n",
    "            sentences_per_file[filename] = num_sentences\n",
    "    return sentences_per_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'output'\n",
    "sentences_per_file = count_sentences_in_directory(directory_path)\n",
    "\n",
    "for filename, num_sentences in sentences_per_file.items():\n",
    "    print(f'{filename}: Number of Sentences: {num_sentences}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_dir = 'cleaned_output'\n",
    "master_dict_dir = 'MasterDictionary'\n",
    "extracted_text_dir = 'output'\n",
    "positive_words_file = os.path.join(master_dict_dir, 'positive-words.txt')\n",
    "negative_words_file = os.path.join(master_dict_dir, 'negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(file_path):\n",
    "    words = set()\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            words.add(line.strip().lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = load_words(positive_words_file)\n",
    "negative_words = load_words(negative_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total positive words: {len(positive_words)}\")\n",
    "print(f\"Total negative words: {len(negative_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(text, positive_words, negative_words):      # Can remove positive_words, negative_words from parameters.\n",
    "    tokens = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(file_path, positive_words):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        positive_score = sum(1 for token in tokens if token in positive_words)\n",
    "        \n",
    "        return positive_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path, positive_words_file):\n",
    "    file_positive_scores = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            positive_score = calculate_positive_score(file_path, positive_words)\n",
    "            file_positive_scores[filename] = positive_score\n",
    "    return file_positive_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output' \n",
    "positive_words_file = 'positive-words.txt'\n",
    "file_positive_scores = process_files_in_directory(directory_path, positive_words_file)\n",
    "\n",
    "for filename, positive_score in file_positive_scores.items():\n",
    "    print(f\"{filename}: Positive score = {positive_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_positive_scores:\n",
    "        df.loc[index, 'POSITIVE SCORE'] = file_positive_scores[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_negative_score(file_path, negative_words):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        negative_score = sum(-1 for token in tokens if token in negative_words)\n",
    "        \n",
    "        return negative_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path, negative_words_file):\n",
    "    file_negative_scores = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            negative_score = calculate_negative_score(file_path, negative_words)\n",
    "            file_negative_scores[filename] = -1 * negative_score\n",
    "    return file_negative_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output' \n",
    "negative_words_file = 'negative-words.txt' \n",
    "file_negative_scores = process_files_in_directory(directory_path, negative_words_file)\n",
    "\n",
    "for filename, negative_score in file_negative_scores.items():\n",
    "    print(f\"{filename}: Negative score = {negative_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_negative_scores:\n",
    "        df.loc[index, 'NEGATIVE SCORE'] = file_negative_scores[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(word_dict_path):\n",
    "    with open(word_dict_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        words = set(word.strip() for word in file.readlines())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(file_path, positive_words_, negative_words_):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read().lower() \n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        positive_score_= sum(1 for word in words if word in positive_words_)\n",
    "        negative_score_ = sum(1 for word in words if word in negative_words_)\n",
    "        return positive_score_, negative_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarity_score(positive_score_ , negative_score_):\n",
    "    denominator = (positive_score_ + negative_score_) + 0.000001\n",
    "    polarity_score = (positive_score_ - negative_score_) / denominator\n",
    "    return polarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path, positive_dict_path, negative_dict_path):\n",
    "    positive_words_ = load_words(positive_dict_path)\n",
    "    negative_words_ = load_words(negative_dict_path)\n",
    "    \n",
    "    file_polarity_scores = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            positive_score_, negative_score_ = calculate_scores(file_path, positive_words_, negative_words_)\n",
    "            polarity_score = calculate_polarity_score(positive_score_, negative_score_)\n",
    "            file_polarity_scores[filename] = polarity_score\n",
    "    return file_polarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output'\n",
    "positive_dict_path = r'MasterDictionary\\positive-words.txt'  \n",
    "negative_dict_path = r'MasterDictionary\\negative-words.txt' \n",
    "\n",
    "file_polarity_scores = process_files_in_directory(directory_path, positive_dict_path, negative_dict_path)\n",
    "\n",
    "for filename, polarity_score in file_polarity_scores.items():\n",
    "    print(f\"{filename}: Polarity score = {polarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_polarity_scores:\n",
    "        df.loc[index, 'POLARITY SCORE'] = file_polarity_scores[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjectivity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        word_list = set(word.strip() for word in file.readlines())\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_score(file_path, positive_words):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read().lower()  \n",
    "        words = re.findall(r'\\b\\w+\\b', text) \n",
    "        positive_score = sum(1 for word in words if word in positive_words)\n",
    "        return positive_score\n",
    "\n",
    "def get_negative_score(file_path, negative_words):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read().lower() \n",
    "        words = re.findall(r'\\b\\w+\\b', text) \n",
    "        negative_score = sum(1 for word in words if word in negative_words)\n",
    "        return negative_score\n",
    "\n",
    "def get_total_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "      \n",
    "        words = re.findall(r'\\b\\w+\\b', text) \n",
    "        total_words = len(words)\n",
    "        return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "    epsilon = 0.000001\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words + epsilon)\n",
    "    return subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path, positive_dict_path, negative_dict_path):\n",
    "    positive_words = load_word_list(positive_dict_path)\n",
    "    negative_words = load_word_list(negative_dict_path)\n",
    "    \n",
    "    file_subjectivity_scores = {}\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            positive_score = get_positive_score(file_path, positive_words)\n",
    "            negative_score = get_negative_score(file_path, negative_words)\n",
    "            total_words = get_total_words(file_path)\n",
    "            \n",
    "            subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)\n",
    "            \n",
    "            file_subjectivity_scores[filename] = subjectivity_score\n",
    "    \n",
    "    return file_subjectivity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output' \n",
    "positive_dict_path = r'MasterDictionary\\positive-words.txt' \n",
    "negative_dict_path = r'MasterDictionary\\negative-words.txt' \n",
    "\n",
    "file_subjectivity_scores = process_files_in_directory(directory_path, positive_dict_path, negative_dict_path)\n",
    "\n",
    "for filename, subjectivity_score in file_subjectivity_scores.items():\n",
    "    print(f\"{filename}: Subjectivity Score = {subjectivity_score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_subjectivity_scores:\n",
    "        df.loc[index, 'SUBJECTIVITY SCORE'] = file_subjectivity_scores[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average number of words per sentence =  the total number of words / the total number of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_words_per_sentence(directory_path):\n",
    "    file_avg_words_per_sentence = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            word_count_ = count_words_in_file(file_path)\n",
    "            num_sentences = count_sentences(file_path)\n",
    "            average_length = word_count_ / num_sentences\n",
    "            file_avg_words_per_sentence[filename] = average_length\n",
    "    return file_avg_words_per_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output'\n",
    "directory_path2 = 'output'\n",
    "\n",
    "file_avg_words_per_sentence = calculate_average_words_per_sentence(directory_path2)\n",
    "\n",
    "print(\"\\nAverage number of words per sentence:\")\n",
    "for filename, average_length in file_avg_words_per_sentence.items():\n",
    "    print(f'{filename}: {average_length:.2f} words per sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_avg_words_per_sentence:\n",
    "        df.loc[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = file_avg_words_per_sentence[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sentence length = the number of words / the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentence_length(words_directory_path, sentences_directory_path):\n",
    "    file_avg_sentence_length = {}\n",
    "    word_counts = count_words_in_directory(words_directory_path)\n",
    "    sentence_counts = count_sentences_in_directory(sentences_directory_path)\n",
    "\n",
    "    for filename in sentence_counts:\n",
    "        word_count = word_counts.get(filename)  \n",
    "        num_sentences = sentence_counts[filename]\n",
    "        \n",
    "        average_sentence_length = word_count / num_sentences\n",
    "        \n",
    "        file_avg_sentence_length[filename] = average_sentence_length\n",
    "    return file_avg_sentence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_directory_path = 'cleaned_output'\n",
    "sentences_directory_path = 'output'\n",
    "\n",
    "file_avg_sentence_length = calculate_average_sentence_length(words_directory_path, sentences_directory_path)\n",
    "\n",
    "print(\"\\nAverage Sentence Length:\")\n",
    "for filename, average_sentence_length in file_avg_sentence_length.items():\n",
    "    print(f'{filename}: {average_sentence_length:.2f} words per sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_avg_sentence_length:\n",
    "        df.loc[index, 'AVG SENTENCE LENGTH'] = file_avg_sentence_length[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syllable count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    exceptions = ['es', 'ed']\n",
    "    word = word.lower()\n",
    "    for ending in exceptions:\n",
    "        if word.endswith(ending):\n",
    "            return 0\n",
    "    \n",
    "    vowels = \"aeiou\"\n",
    "    count = 0\n",
    "    last_char_was_vowel = False\n",
    "    \n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            if not last_char_was_vowel:\n",
    "                count += 1\n",
    "            last_char_was_vowel = True\n",
    "        else:\n",
    "            last_char_was_vowel = False\n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables_in_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        total_syllables = sum(count_syllables(word) for word in words)\n",
    "        return total_syllables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path):\n",
    "    file_syllable_counts = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            syllable_count = count_syllables_in_file(file_path)\n",
    "            file_syllable_counts[filename] = syllable_count\n",
    "    return file_syllable_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output'\n",
    "file_syllable_counts = process_files_in_directory(directory_path)\n",
    "\n",
    "for filename, syllable_count in file_syllable_counts.items():\n",
    "    print(f\"{filename}: {syllable_count} syllables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syllable count per word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_syllable_per_word(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        total_words_ = len(words)\n",
    "        total_syllables = sum(count_syllables(word) for word in words)\n",
    "        \n",
    "        syllable_per_word = total_syllables / total_words_\n",
    "        \n",
    "        return syllable_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path):\n",
    "    file_syllable_count_per_word = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            syllable_per_word = calculate_syllable_per_word(file_path)\n",
    "            file_syllable_count_per_word[filename] = syllable_per_word\n",
    "    return file_syllable_count_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output' \n",
    "file_syllable_count_per_word = process_files_in_directory(directory_path)\n",
    "\n",
    "for filename, syllable_per_word in file_syllable_count_per_word.items():\n",
    "    print(f\"{filename}: {syllable_per_word:.2f} syllables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_syllable_count_per_word:\n",
    "        df.loc[index, 'SYLLABLE PER WORD'] = file_syllable_count_per_word[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex words count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_complex_words_in_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        complex_word_count = sum(1 for word in words if count_syllables(word) > 2)\n",
    "        return complex_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path):\n",
    "    file_complex_word_counts = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            complex_word_count = count_complex_words_in_file(file_path)\n",
    "            file_complex_word_counts[filename] = complex_word_count\n",
    "    return file_complex_word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output' \n",
    "file_complex_word_counts = process_files_in_directory(directory_path)\n",
    "\n",
    "for filename, complex_word_count in file_complex_word_counts.items():\n",
    "    print(f\"{filename}: {complex_word_count} complex words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\"\n",
    "    if filename in file_complex_word_counts:\n",
    "        df.loc[index, 'COMPLEX WORD COUNT'] = file_complex_word_counts[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_of_complex_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        complex_word_count = sum(1 for word in words if count_syllables(word) > 2)\n",
    "        total_words = len(words)\n",
    "        \n",
    "        percentage_of_complex_words = (complex_word_count / total_words) * 100\n",
    "      \n",
    "        return percentage_of_complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path):\n",
    "    file_complex_percentages = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            percentage_of_complex_words = calculate_percentage_of_complex_words(file_path)\n",
    "            file_complex_percentages[filename] = percentage_of_complex_words\n",
    "    return file_complex_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'cleaned_output'\n",
    "file_complex_percentages = process_files_in_directory(directory_path)\n",
    "\n",
    "for filename, percentage_of_complex_words in file_complex_percentages.items():\n",
    "    print(f\"{filename}: Percentage of Complex Words = {percentage_of_complex_words:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\"  \n",
    "    if filename in file_complex_percentages:\n",
    "        df.loc[index, 'PERCENTAGE OF COMPLEX WORDS'] = file_complex_percentages[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fog_index(words_directory_path, sentences_directory_path):\n",
    "    avg_sentence_lengths = calculate_average_sentence_length(words_directory_path, sentences_directory_path)\n",
    "    fog_indices = {}\n",
    "    \n",
    "    for filename in avg_sentence_lengths:\n",
    "        avg_sentence_length = avg_sentence_lengths[filename]\n",
    "        file_path = os.path.join(words_directory_path, filename)\n",
    "        percent_complex_words = calculate_percentage_of_complex_words(file_path)\n",
    "        fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
    "        fog_indices[filename] = fog_index\n",
    "    \n",
    "    return fog_indices\n",
    "\n",
    "words_directory_path = 'cleaned_output'\n",
    "sentences_directory_path = 'output'\n",
    "fog_indices = calculate_fog_index(words_directory_path, sentences_directory_path)\n",
    "\n",
    "for filename, fog_index in fog_indices.items():\n",
    "    print(f'Fog Index for {filename}: {fog_index:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in fog_indices:\n",
    "        df.loc[index, 'FOG INDEX'] = fog_indices[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal pronouns in output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    pronoun_pattern = r'\\b(I|we|my|ours|us)\\b'\n",
    "    pronouns = re.findall(pronoun_pattern, text, re.IGNORECASE)\n",
    "    filtered_pronouns = [pronoun for pronoun in pronouns if pronoun.lower() != 'us' or not re.search(r'\\bUS\\b', text)]\n",
    "    return len(filtered_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_personal_pronouns(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "        total_pronouns = count_personal_pronouns(text)\n",
    "        return total_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path):\n",
    "    files_personal_pronoun_count = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            total_pronouns = calculate_personal_pronouns(file_path)\n",
    "            files_personal_pronoun_count[filename] = total_pronouns\n",
    "    \n",
    "    return files_personal_pronoun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'output' \n",
    "file_personal_pronoun_count = process_files_in_directory(directory_path)\n",
    "\n",
    "for filename, total_pronouns in file_personal_pronoun_count.items():\n",
    "    print(f\"{filename}: Total Personal Pronouns = {total_pronouns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\" \n",
    "    if filename in file_personal_pronoun_count:\n",
    "        df.loc[index, 'PERSONAL PRONOUNS'] = file_personal_pronoun_count[filename]\n",
    "\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
